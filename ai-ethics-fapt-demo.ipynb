{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f46c3c2-efc7-4953-a0d6-e0fb5b2ca314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PTAF Demo Complete ===\n",
      "Artifacts written to: D:\\Scripts\\Python Scripts\\Responsible AI\\artifacts\n",
      "\n",
      "Key files:\n",
      " - model_card.json\n",
      " - coefficients.csv\n",
      " - sample_explanations.json\n",
      " - audit_log.jsonl\n",
      " - fairness_summary.json\n",
      " - fairness_subgroup_metrics_baseline.csv\n",
      " - fairness_subgroup_metrics_mitigated.csv\n",
      " - dp_like_aggregates.json\n",
      " - k_anonymity_group_sizes.csv\n",
      "\n",
      "Headline metrics:\n",
      " - Accuracy:  0.620\n",
      " - Precision: 0.455\n",
      " - Recall:    0.180\n",
      "\n",
      "Fairness (baseline):\n",
      " - Disparate Impact (g1/g0): 1.0408850408850407\n",
      " - Equal Opportunity Diff:   0.025\n",
      "\n",
      "Fairness (mitigated demo):\n",
      " - Thresholds: group0=0.45, group1=0.45\n",
      " - Disparate Impact (g1/g0): 1.0027314408350405\n",
      " - Equal Opportunity Diff:   0.024\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "AI Ethics in Action — Privacy, Transparency, Accountability, Fairness (PTAF)\n",
    "\n",
    "A compact, end-to-end, portfolio-friendly demo that:\n",
    "  1) builds a synthetic dataset with identifiers + quasi-identifiers + a sensitive attribute\n",
    "  2) applies privacy protections (pseudonymization + generalization + DP-like aggregates)\n",
    "  3) trains an interpretable model and generates transparency artifacts (coefficients, reason codes, model card)\n",
    "  4) logs accountability artifacts (audit log, dataset/model fingerprints, HITL escalation)\n",
    "  5) evaluates fairness (subgroup metrics, disparate impact, equal opportunity) and applies a simple mitigation\n",
    "  6) exports outputs to ./artifacts for easy GitHub sharing\n",
    "\n",
    "Run:\n",
    "  python ethics_in_action.py\n",
    "\n",
    "Optional:\n",
    "  python ethics_in_action.py --n 6000 --seed 7 --threshold 0.55\n",
    "\n",
    "Dependencies:\n",
    "  pip install numpy pandas scikit-learn\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import hashlib\n",
    "import json\n",
    "import os\n",
    "from dataclasses import asdict, dataclass\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Utilities\n",
    "# ----------------------------\n",
    "\n",
    "def now_iso() -> str:\n",
    "    return datetime.now().isoformat(timespec=\"seconds\")\n",
    "\n",
    "\n",
    "def ensure_dir(path: Path) -> None:\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def write_json(path: Path, payload: Any) -> None:\n",
    "    path.write_text(json.dumps(payload, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "def write_jsonl(path: Path, events: List[Dict[str, Any]]) -> None:\n",
    "    with path.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        for e in events:\n",
    "            f.write(json.dumps(e, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Data Generation (Synthetic)\n",
    "# ----------------------------\n",
    "\n",
    "def make_synthetic_hiring_data(n: int, seed: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates a synthetic 'hiring screen' dataset.\n",
    "\n",
    "    Columns include:\n",
    "      - direct identifiers: name, email (to demonstrate privacy controls)\n",
    "      - quasi-identifiers: age, zipcode, education (re-identification risk)\n",
    "      - features: experience_years, skill_score\n",
    "      - sensitive attribute: group (0/1) used for fairness evaluation\n",
    "      - label: recommended_for_review (0/1)\n",
    "\n",
    "    Note: This is a demo dataset for learning and portfolio use.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    age = rng.integers(21, 60, size=n)\n",
    "    zipcode = rng.choice([10001, 10002, 10003, 10004, 10005, 10006, 10007, 10008], size=n)\n",
    "    education = rng.choice(\n",
    "        [\"HighSchool\", \"Bachelors\", \"Masters\", \"PhD\"],\n",
    "        size=n,\n",
    "        p=[0.20, 0.45, 0.28, 0.07],\n",
    "    )\n",
    "    experience_years = np.clip(rng.normal(loc=(age - 21) / 3, scale=2.5, size=n), 0, 30)\n",
    "    skill_score = np.clip(rng.normal(loc=65, scale=12, size=n), 0, 100)\n",
    "\n",
    "    # Sensitive attribute (e.g., a protected demographic group)\n",
    "    group = rng.binomial(1, 0.45, size=n)\n",
    "\n",
    "    edu_map = {\"HighSchool\": 0, \"Bachelors\": 1, \"Masters\": 2, \"PhD\": 3}\n",
    "    edu_num = np.array([edu_map[e] for e in education])\n",
    "\n",
    "    # Signal from \"legitimate\" features\n",
    "    base = 0.03 * skill_score + 0.09 * experience_years + 0.25 * edu_num\n",
    "\n",
    "    # Introduce mild bias in label generation (for fairness demonstration)\n",
    "    biased_score = base - 0.25 * group + rng.normal(0, 0.8, size=n)\n",
    "\n",
    "    # Convert to probability and sample label\n",
    "    prob = 1 / (1 + np.exp(-(biased_score - 3.4)))\n",
    "    y = rng.binomial(1, prob, size=n)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"name\": [f\"Candidate_{i}\" for i in range(n)],\n",
    "            \"email\": [f\"user{i}@example.com\" for i in range(n)],\n",
    "            \"age\": age,\n",
    "            \"zipcode\": zipcode,\n",
    "            \"education\": education,\n",
    "            \"experience_years\": np.round(experience_years, 1),\n",
    "            \"skill_score\": np.round(skill_score, 1),\n",
    "            \"group\": group,\n",
    "            \"recommended_for_review\": y,\n",
    "        }\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Privacy\n",
    "# ----------------------------\n",
    "\n",
    "def pseudonymize(df: pd.DataFrame, salt: str, id_cols: Tuple[str, ...] = (\"name\", \"email\")) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Privacy control: pseudonymization\n",
    "      - create a stable pseudo_id using salted hashing\n",
    "      - drop direct identifiers (name/email)\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "\n",
    "    def pid(email: str) -> str:\n",
    "        return hashlib.sha256((salt + str(email)).encode(\"utf-8\")).hexdigest()[:12]\n",
    "\n",
    "    out[\"pseudo_id\"] = out[\"email\"].map(pid)\n",
    "    out = out.drop(columns=list(id_cols))\n",
    "    cols = [\"pseudo_id\"] + [c for c in out.columns if c != \"pseudo_id\"]\n",
    "    return out[cols]\n",
    "\n",
    "\n",
    "def generalize_quasi_identifiers(\n",
    "    df: pd.DataFrame,\n",
    "    k: int = 15,\n",
    "    quasi: Tuple[str, ...] = (\"age\", \"zipcode\", \"education\"),\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Privacy control: generalization (k-anonymity style)\n",
    "      - age -> age_band (5-year buckets)\n",
    "      - zipcode -> zip_prefix (first 3 digits + \"**\")\n",
    "      - keeps education as-is (often used as a quasi-identifier)\n",
    "    Returns:\n",
    "      - generalized dataframe\n",
    "      - group size table for the quasi-identifier combinations\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "\n",
    "    out[\"age_band_start\"] = (out[\"age\"] // 5) * 5\n",
    "    out[\"age_band\"] = out[\"age_band_start\"].astype(str) + \"-\" + (out[\"age_band_start\"] + 4).astype(str)\n",
    "    out[\"zip_prefix\"] = out[\"zipcode\"].astype(str).str[:3] + \"**\"\n",
    "\n",
    "    # drop exact quasi-identifiers that increase re-identification risk\n",
    "    out = out.drop(columns=[\"age\", \"zipcode\", \"age_band_start\"])\n",
    "\n",
    "    # group-size table\n",
    "    group_sizes = (\n",
    "        out.groupby([\"age_band\", \"zip_prefix\", \"education\"])\n",
    "        .size()\n",
    "        .reset_index(name=\"count\")\n",
    "        .sort_values(\"count\")\n",
    "    )\n",
    "    # the caller can inspect how many groups fall below k\n",
    "    return out, group_sizes\n",
    "\n",
    "\n",
    "def dp_like_mean(values: np.ndarray, epsilon: float, value_range: Tuple[float, float], seed: int) -> float:\n",
    "    \"\"\"\n",
    "    Privacy control: DP-like aggregate release (teaching demo)\n",
    "      - sensitivity(mean) ≈ (max-min)/n\n",
    "      - Laplace noise with scale = sensitivity/epsilon\n",
    "\n",
    "    Notes:\n",
    "      - This is NOT a production DP implementation.\n",
    "      - It demonstrates the concept of adding calibrated noise before sharing aggregates.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    values = np.asarray(values, dtype=float)\n",
    "    n = max(len(values), 1)\n",
    "    sensitivity = (value_range[1] - value_range[0]) / n\n",
    "    noise = rng.laplace(loc=0.0, scale=sensitivity / max(epsilon, 1e-9))\n",
    "    return float(values.mean() + noise)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Transparency\n",
    "# ----------------------------\n",
    "\n",
    "def build_pipeline(cat_cols: List[str], num_cols: List[str]) -> Pipeline:\n",
    "    pre = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "            (\"num\", \"passthrough\", num_cols),\n",
    "        ]\n",
    "    )\n",
    "    clf = LogisticRegression(max_iter=2000)\n",
    "    return Pipeline([(\"prep\", pre), (\"clf\", clf)])\n",
    "\n",
    "\n",
    "def feature_names_from_pipeline(pipe: Pipeline, cat_cols: List[str], num_cols: List[str]) -> List[str]:\n",
    "    pre: ColumnTransformer = pipe.named_steps[\"prep\"]\n",
    "    ohe: OneHotEncoder = pre.named_transformers_[\"cat\"]\n",
    "    cat_names = list(ohe.get_feature_names_out(cat_cols))\n",
    "    return cat_names + num_cols\n",
    "\n",
    "\n",
    "def coefficient_table(pipe: Pipeline, cat_cols: List[str], num_cols: List[str]) -> pd.DataFrame:\n",
    "    names = feature_names_from_pipeline(pipe, cat_cols, num_cols)\n",
    "    coefs = pipe.named_steps[\"clf\"].coef_[0]\n",
    "    tbl = pd.DataFrame({\"feature\": names, \"coef\": coefs, \"abs_coef\": np.abs(coefs)})\n",
    "    return tbl.sort_values(\"abs_coef\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def reason_codes(pipe: Pipeline, row_df: pd.DataFrame, cat_cols: List[str], num_cols: List[str], top_k: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Simple per-decision explanation:\n",
    "      - for logistic regression, contribution ≈ feature_value * coefficient in transformed space\n",
    "    \"\"\"\n",
    "    pre: ColumnTransformer = pipe.named_steps[\"prep\"]\n",
    "    clf: LogisticRegression = pipe.named_steps[\"clf\"]\n",
    "\n",
    "    x_vec = pre.transform(row_df)\n",
    "\n",
    "    # x_vec might be sparse depending on sklearn; convert safely\n",
    "    x_dense = x_vec.toarray() if hasattr(x_vec, \"toarray\") else np.asarray(x_vec)\n",
    "    contrib = x_dense[0] * clf.coef_[0]\n",
    "\n",
    "    names = feature_names_from_pipeline(pipe, cat_cols, num_cols)\n",
    "    out = pd.DataFrame({\"feature\": names, \"contribution\": contrib, \"abs\": np.abs(contrib)})\n",
    "    return out.sort_values(\"abs\", ascending=False).head(top_k)[[\"feature\", \"contribution\"]].reset_index(drop=True)\n",
    "\n",
    "\n",
    "def make_model_card(\n",
    "    model_name: str,\n",
    "    intended_use: str,\n",
    "    limitations: List[str],\n",
    "    data_notes: str,\n",
    "    metrics: Dict[str, float],\n",
    "    ethical_notes: Dict[str, str],\n",
    ") -> Dict[str, Any]:\n",
    "    return {\n",
    "        \"model_name\": model_name,\n",
    "        \"created_at\": now_iso(),\n",
    "        \"intended_use\": intended_use,\n",
    "        \"data_notes\": data_notes,\n",
    "        \"limitations\": limitations,\n",
    "        \"evaluation_metrics\": metrics,\n",
    "        \"ethical_notes\": ethical_notes,\n",
    "    }\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Accountability\n",
    "# ----------------------------\n",
    "\n",
    "def df_fingerprint(df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Stable-ish fingerprint for dataset content.\n",
    "    Useful for traceability: \"which data version trained this model?\"\n",
    "    \"\"\"\n",
    "    h = pd.util.hash_pandas_object(df, index=True).values.tobytes()\n",
    "    return hashlib.sha256(h).hexdigest()\n",
    "\n",
    "\n",
    "def pipeline_fingerprint(pipe: Pipeline) -> str:\n",
    "    \"\"\"\n",
    "    Fingerprint model configuration (parameters).\n",
    "    For demos: captures a stable signature of the pipeline setup.\n",
    "    \"\"\"\n",
    "    params = pipe.get_params(deep=True)\n",
    "    blob = json.dumps(params, sort_keys=True, default=str).encode(\"utf-8\")\n",
    "    return hashlib.sha256(blob).hexdigest()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AuditEvent:\n",
    "    timestamp: str\n",
    "    event_type: str\n",
    "    payload: Dict[str, Any]\n",
    "\n",
    "\n",
    "class Auditor:\n",
    "    def __init__(self, audit_path: Path):\n",
    "        self.audit_path = audit_path\n",
    "        self.buffer: List[Dict[str, Any]] = []\n",
    "\n",
    "    def log(self, event_type: str, payload: Dict[str, Any]) -> None:\n",
    "        self.buffer.append({\"timestamp\": now_iso(), \"event_type\": event_type, \"payload\": payload})\n",
    "\n",
    "    def flush(self) -> None:\n",
    "        if self.buffer:\n",
    "            write_jsonl(self.audit_path, self.buffer)\n",
    "            self.buffer = []\n",
    "\n",
    "\n",
    "def predict_with_audit(\n",
    "    auditor: Auditor,\n",
    "    pipe: Pipeline,\n",
    "    row_df: pd.DataFrame,\n",
    "    cat_cols: List[str],\n",
    "    num_cols: List[str],\n",
    "    reviewer: str,\n",
    "    threshold: float,\n",
    ") -> Dict[str, Any]:\n",
    "    p = float(pipe.predict_proba(row_df)[0, 1])\n",
    "    decision = \"REVIEW\" if p >= threshold else \"NO_REVIEW\"\n",
    "    reasons = reason_codes(pipe, row_df, cat_cols, num_cols, top_k=4).to_dict(orient=\"records\")\n",
    "\n",
    "    auditor.log(\n",
    "        \"prediction\",\n",
    "        {\n",
    "            \"reviewer\": reviewer,\n",
    "            \"threshold\": threshold,\n",
    "            \"probability\": p,\n",
    "            \"decision\": decision,\n",
    "            \"input_features\": row_df.to_dict(orient=\"records\")[0],\n",
    "            \"reason_codes\": reasons,\n",
    "        },\n",
    "    )\n",
    "    return {\"probability\": p, \"decision\": decision, \"reason_codes\": reasons}\n",
    "\n",
    "\n",
    "def hitl_policy(probability: float, low: float = 0.45, high: float = 0.65) -> str:\n",
    "    \"\"\"\n",
    "    Human-in-the-loop escalation policy:\n",
    "      - low confidence: auto NO_REVIEW\n",
    "      - high confidence: auto REVIEW\n",
    "      - ambiguous: escalate to human\n",
    "    \"\"\"\n",
    "    if probability < low:\n",
    "        return \"AUTO_NO_REVIEW\"\n",
    "    if probability > high:\n",
    "        return \"AUTO_REVIEW\"\n",
    "    return \"ESCALATE_TO_HUMAN\"\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Fairness\n",
    "# ----------------------------\n",
    "\n",
    "def subgroup_report(y_true: np.ndarray, y_pred: np.ndarray, sensitive: np.ndarray) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for g in sorted(np.unique(sensitive)):\n",
    "        idx = sensitive == g\n",
    "        rows.append(\n",
    "            {\n",
    "                \"group\": int(g),\n",
    "                \"n\": int(idx.sum()),\n",
    "                \"accuracy\": float(accuracy_score(y_true[idx], y_pred[idx])),\n",
    "                \"precision\": float(precision_score(y_true[idx], y_pred[idx], zero_division=0)),\n",
    "                \"recall\": float(recall_score(y_true[idx], y_pred[idx], zero_division=0)),\n",
    "                \"selection_rate\": float(y_pred[idx].mean()),\n",
    "            }\n",
    "        )\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def disparate_impact(y_pred: np.ndarray, sensitive: np.ndarray) -> Tuple[Optional[float], Dict[int, float]]:\n",
    "    rates: Dict[int, float] = {}\n",
    "    for g in sorted(np.unique(sensitive)):\n",
    "        idx = sensitive == g\n",
    "        rates[int(g)] = float(y_pred[idx].mean())\n",
    "    if 0 in rates and rates[0] > 0 and 1 in rates:\n",
    "        return float(rates[1] / rates[0]), rates\n",
    "    return None, rates\n",
    "\n",
    "\n",
    "def equal_opportunity_diff(y_true: np.ndarray, y_pred: np.ndarray, sensitive: np.ndarray) -> Tuple[float, Dict[int, float]]:\n",
    "    tpr: Dict[int, float] = {}\n",
    "    for g in sorted(np.unique(sensitive)):\n",
    "        idx = sensitive == g\n",
    "        tpr[int(g)] = float(recall_score(y_true[idx], y_pred[idx], zero_division=0))\n",
    "    return float(tpr.get(1, 0.0) - tpr.get(0, 0.0)), tpr\n",
    "\n",
    "\n",
    "def apply_group_thresholds(proba: np.ndarray, sensitive: np.ndarray, t0: float, t1: float) -> np.ndarray:\n",
    "    out = np.zeros_like(proba, dtype=int)\n",
    "    out[sensitive == 0] = (proba[sensitive == 0] >= t0).astype(int)\n",
    "    out[sensitive == 1] = (proba[sensitive == 1] >= t1).astype(int)\n",
    "    return out\n",
    "\n",
    "\n",
    "def search_thresholds(\n",
    "    proba: np.ndarray,\n",
    "    y_true: np.ndarray,\n",
    "    sensitive: np.ndarray,\n",
    "    grid: np.ndarray,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Simple grid search for thresholds that pull DI closer to 1.0\n",
    "    while keeping accuracy reasonable.\n",
    "    \"\"\"\n",
    "    best: Optional[Dict[str, float]] = None\n",
    "    for t0 in grid:\n",
    "        for t1 in grid:\n",
    "            pred = apply_group_thresholds(proba, sensitive, float(t0), float(t1))\n",
    "            di, _ = disparate_impact(pred, sensitive)\n",
    "            if di is None:\n",
    "                continue\n",
    "            acc = float(accuracy_score(y_true, pred))\n",
    "            score = -abs(di - 1.0) + 0.15 * acc  # lightweight objective for demo\n",
    "            cand = {\"t0\": float(t0), \"t1\": float(t1), \"di\": float(di), \"acc\": acc, \"score\": float(score)}\n",
    "            if best is None or cand[\"score\"] > best[\"score\"]:\n",
    "                best = cand\n",
    "    return best or {\"t0\": 0.5, \"t1\": 0.5, \"di\": float(\"nan\"), \"acc\": 0.0, \"score\": float(\"-inf\")}\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Main workflow\n",
    "# ----------------------------\n",
    "\n",
    "def main() -> None:\n",
    "    parser = argparse.ArgumentParser(description=\"AI Ethics in Action (PTAF) demo.\")\n",
    "    parser.add_argument(\"--n\", type=int, default=4000, help=\"Number of rows to generate.\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=42, help=\"Random seed.\")\n",
    "    parser.add_argument(\"--threshold\", type=float, default=0.50, help=\"Decision threshold for REVIEW.\")\n",
    "    parser.add_argument(\"--epsilon\", type=float, default=0.8, help=\"DP-like epsilon for aggregate demos.\")\n",
    "    parser.add_argument(\"--k\", type=int, default=15, help=\"k for k-anonymity style generalization check.\")\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "\n",
    "    artifacts = Path(\"artifacts\")\n",
    "    ensure_dir(artifacts)\n",
    "\n",
    "    # 1) Data\n",
    "    df_raw = make_synthetic_hiring_data(n=args.n, seed=args.seed)\n",
    "    df_raw.to_csv(artifacts / \"raw_data.csv\", index=False)\n",
    "\n",
    "    # 2) Privacy: pseudonymize + generalize\n",
    "    salt = f\"PTAF_DEMO_SALT_{args.seed}\"\n",
    "    df_priv = pseudonymize(df_raw, salt=salt)\n",
    "    df_priv.to_csv(artifacts / \"pseudonymized_data.csv\", index=False)\n",
    "\n",
    "    df_gen, group_sizes = generalize_quasi_identifiers(df_priv, k=args.k)\n",
    "    df_gen.to_csv(artifacts / \"generalized_data.csv\", index=False)\n",
    "    group_sizes.to_csv(artifacts / \"k_anonymity_group_sizes.csv\", index=False)\n",
    "\n",
    "    # DP-like aggregates\n",
    "    dp_report = []\n",
    "    for g in [0, 1]:\n",
    "        mask = df_priv[\"group\"].values == g\n",
    "        real_mean = float(df_raw.loc[mask, \"skill_score\"].mean())\n",
    "        dp_mean = dp_like_mean(\n",
    "            values=df_raw.loc[mask, \"skill_score\"].values,\n",
    "            epsilon=args.epsilon,\n",
    "            value_range=(0.0, 100.0),\n",
    "            seed=args.seed + 100 + g,\n",
    "        )\n",
    "        dp_report.append({\"group\": g, \"real_mean_skill\": real_mean, \"dp_like_mean_skill\": dp_mean})\n",
    "    write_json(artifacts / \"dp_like_aggregates.json\", {\"created_at\": now_iso(), \"epsilon\": args.epsilon, \"items\": dp_report})\n",
    "\n",
    "    # 3) Train interpretable model (transparency)\n",
    "    # Use privacy-safe columns; do NOT use pseudo_id or group as features\n",
    "    y = df_priv[\"recommended_for_review\"].values.astype(int)\n",
    "    sensitive = df_priv[\"group\"].values.astype(int)\n",
    "\n",
    "    X = df_priv.drop(columns=[\"recommended_for_review\", \"pseudo_id\", \"group\"])\n",
    "    cat_cols = [\"education\"]\n",
    "    num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "\n",
    "    X_train, X_test, y_train, y_test, s_train, s_test = train_test_split(\n",
    "        X, y, sensitive, test_size=0.25, random_state=args.seed, stratify=y\n",
    "    )\n",
    "\n",
    "    pipe = build_pipeline(cat_cols, num_cols)\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    proba = pipe.predict_proba(X_test)[:, 1]\n",
    "    pred = (proba >= args.threshold).astype(int)\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": float(accuracy_score(y_test, pred)),\n",
    "        \"precision\": float(precision_score(y_test, pred)),\n",
    "        \"recall\": float(recall_score(y_test, pred)),\n",
    "        \"threshold\": float(args.threshold),\n",
    "    }\n",
    "    write_json(artifacts / \"metrics.json\", {\"created_at\": now_iso(), \"metrics\": metrics})\n",
    "\n",
    "    # Transparency artifacts: coefficients + sample explanations + model card\n",
    "    coef_tbl = coefficient_table(pipe, cat_cols, num_cols)\n",
    "    coef_tbl.to_csv(artifacts / \"coefficients.csv\", index=False)\n",
    "\n",
    "    explanations = []\n",
    "    sample_indices = np.linspace(0, len(X_test) - 1, num=min(12, len(X_test)), dtype=int)\n",
    "    for idx in sample_indices:\n",
    "        row = X_test.iloc[[idx]]\n",
    "        p = float(pipe.predict_proba(row)[0, 1])\n",
    "        reasons_tbl = reason_codes(pipe, row, cat_cols, num_cols, top_k=5)\n",
    "        explanations.append(\n",
    "            {\n",
    "                \"row_index\": int(idx),\n",
    "                \"probability\": p,\n",
    "                \"decision\": \"REVIEW\" if p >= args.threshold else \"NO_REVIEW\",\n",
    "                \"reason_codes\": reasons_tbl.to_dict(orient=\"records\"),\n",
    "                \"input\": row.to_dict(orient=\"records\")[0],\n",
    "            }\n",
    "        )\n",
    "    write_json(artifacts / \"sample_explanations.json\", {\"created_at\": now_iso(), \"items\": explanations})\n",
    "\n",
    "    model_card = make_model_card(\n",
    "        model_name=\"HiringScreen-PTAF-v1 (LogisticRegression)\",\n",
    "        intended_use=\"Assist with screening by recommending candidates for human review (not an automated hiring decision).\",\n",
    "        limitations=[\n",
    "            \"Synthetic dataset used for demonstration; not representative of real labor markets.\",\n",
    "            \"Model behavior depends on feature definitions; requires domain review before any real use.\",\n",
    "            \"Interpretability is limited to linear contributions; does not capture complex interactions.\",\n",
    "        ],\n",
    "        data_notes=\"Trained on education, experience_years, skill_score; direct identifiers removed; sensitive attribute used for evaluation only.\",\n",
    "        metrics=metrics,\n",
    "        ethical_notes={\n",
    "            \"privacy\": \"Direct identifiers removed; pseudonymous IDs used; quasi-identifiers generalized for sharing.\",\n",
    "            \"transparency\": \"Coefficients and per-decision reason codes exported; model card included.\",\n",
    "            \"accountability\": \"Audit logs + dataset/model fingerprints; HITL escalation policy demonstrated.\",\n",
    "            \"fairness\": \"Subgroup metrics, disparate impact, and equal opportunity reported; threshold mitigation demonstrated.\",\n",
    "        },\n",
    "    )\n",
    "    write_json(artifacts / \"model_card.json\", model_card)\n",
    "\n",
    "    # 4) Accountability: audit + fingerprints + HITL policy\n",
    "    auditor = Auditor(audit_path=artifacts / \"audit_log.jsonl\")\n",
    "\n",
    "    train_fprint = df_fingerprint(pd.concat([X_train.reset_index(drop=True), pd.Series(y_train, name=\"label\")], axis=1))\n",
    "    model_fprint = pipeline_fingerprint(pipe)\n",
    "\n",
    "    auditor.log(\n",
    "        \"model_registry\",\n",
    "        {\n",
    "            \"model_name\": model_card[\"model_name\"],\n",
    "            \"dataset_fingerprint\": train_fprint,\n",
    "            \"model_fingerprint\": model_fprint,\n",
    "            \"metrics\": metrics,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Log a few audited predictions\n",
    "    for idx in sample_indices[:6]:\n",
    "        row = X_test.iloc[[idx]]\n",
    "        out = predict_with_audit(\n",
    "            auditor=auditor,\n",
    "            pipe=pipe,\n",
    "            row_df=row,\n",
    "            cat_cols=cat_cols,\n",
    "            num_cols=num_cols,\n",
    "            reviewer=\"demo_user\",\n",
    "            threshold=args.threshold,\n",
    "        )\n",
    "        auditor.log(\n",
    "            \"hitl_policy\",\n",
    "            {\n",
    "                \"row_index\": int(idx),\n",
    "                \"probability\": out[\"probability\"],\n",
    "                \"action\": hitl_policy(out[\"probability\"]),\n",
    "            },\n",
    "        )\n",
    "\n",
    "    auditor.flush()\n",
    "\n",
    "    # 5) Fairness: baseline metrics + mitigation via threshold tuning\n",
    "    base_subgroups = subgroup_report(y_test, pred, s_test)\n",
    "    base_subgroups.to_csv(artifacts / \"fairness_subgroup_metrics_baseline.csv\", index=False)\n",
    "\n",
    "    di, sel_rates = disparate_impact(pred, s_test)\n",
    "    eod, tpr = equal_opportunity_diff(y_test, pred, s_test)\n",
    "\n",
    "    fairness_summary = {\n",
    "        \"created_at\": now_iso(),\n",
    "        \"baseline\": {\n",
    "            \"threshold\": float(args.threshold),\n",
    "            \"selection_rates\": sel_rates,\n",
    "            \"disparate_impact_ratio_g1_over_g0\": di,\n",
    "            \"tpr_by_group\": tpr,\n",
    "            \"equal_opportunity_diff_g1_minus_g0\": eod,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Mitigation: allow different thresholds per group (demo)\n",
    "    grid = np.linspace(0.35, 0.65, 13)\n",
    "    best = search_thresholds(proba, y_test, s_test, grid=grid)\n",
    "\n",
    "    pred_m = apply_group_thresholds(proba, s_test, best[\"t0\"], best[\"t1\"])\n",
    "    mitigated_subgroups = subgroup_report(y_test, pred_m, s_test)\n",
    "    mitigated_subgroups.to_csv(artifacts / \"fairness_subgroup_metrics_mitigated.csv\", index=False)\n",
    "\n",
    "    di2, sel_rates2 = disparate_impact(pred_m, s_test)\n",
    "    eod2, tpr2 = equal_opportunity_diff(y_test, pred_m, s_test)\n",
    "\n",
    "    fairness_summary[\"mitigated\"] = {\n",
    "        \"thresholds\": {\"group0\": best[\"t0\"], \"group1\": best[\"t1\"]},\n",
    "        \"accuracy\": float(accuracy_score(y_test, pred_m)),\n",
    "        \"selection_rates\": sel_rates2,\n",
    "        \"disparate_impact_ratio_g1_over_g0\": di2,\n",
    "        \"tpr_by_group\": tpr2,\n",
    "        \"equal_opportunity_diff_g1_minus_g0\": eod2,\n",
    "        \"search_objective\": {\"score\": best[\"score\"], \"baseline_acc\": metrics[\"accuracy\"]},\n",
    "        \"note\": \"This mitigation is for demonstration and must be governed carefully in real systems.\",\n",
    "    }\n",
    "\n",
    "    write_json(artifacts / \"fairness_summary.json\", fairness_summary)\n",
    "\n",
    "    # Final console summary\n",
    "    print(\"\\n=== PTAF Demo Complete ===\")\n",
    "    print(f\"Artifacts written to: {artifacts.resolve()}\")\n",
    "    print(\"\\nKey files:\")\n",
    "    for name in [\n",
    "        \"model_card.json\",\n",
    "        \"coefficients.csv\",\n",
    "        \"sample_explanations.json\",\n",
    "        \"audit_log.jsonl\",\n",
    "        \"fairness_summary.json\",\n",
    "        \"fairness_subgroup_metrics_baseline.csv\",\n",
    "        \"fairness_subgroup_metrics_mitigated.csv\",\n",
    "        \"dp_like_aggregates.json\",\n",
    "        \"k_anonymity_group_sizes.csv\",\n",
    "    ]:\n",
    "        p = artifacts / name\n",
    "        if p.exists():\n",
    "            print(f\" - {name}\")\n",
    "\n",
    "    print(\"\\nHeadline metrics:\")\n",
    "    print(f\" - Accuracy:  {metrics['accuracy']:.3f}\")\n",
    "    print(f\" - Precision: {metrics['precision']:.3f}\")\n",
    "    print(f\" - Recall:    {metrics['recall']:.3f}\")\n",
    "    print(\"\\nFairness (baseline):\")\n",
    "    print(f\" - Disparate Impact (g1/g0): {di if di is not None else 'n/a'}\")\n",
    "    print(f\" - Equal Opportunity Diff:   {eod:.3f}\")\n",
    "    print(\"\\nFairness (mitigated demo):\")\n",
    "    print(f\" - Thresholds: group0={best['t0']:.2f}, group1={best['t1']:.2f}\")\n",
    "    print(f\" - Disparate Impact (g1/g0): {di2 if di2 is not None else 'n/a'}\")\n",
    "    print(f\" - Equal Opportunity Diff:   {eod2:.3f}\")\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb1e238-e52e-4866-b4a1-3e6062ac67ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
