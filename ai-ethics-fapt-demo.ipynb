{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f46c3c2-efc7-4953-a0d6-e0fb5b2ca314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PTAF Demo Complete ===\n",
      "Artifacts written to: D:\\Scripts\\Python Scripts\\Responsible AI\\artifacts\n",
      "\n",
      "Key files:\n",
      " - model_card.json\n",
      " - coefficients.csv\n",
      " - sample_explanations.json\n",
      " - audit_log.jsonl\n",
      " - fairness_summary.json\n",
      " - fairness_subgroup_metrics_baseline.csv\n",
      " - fairness_subgroup_metrics_mitigated.csv\n",
      " - dp_like_aggregates.json\n",
      " - k_anonymity_group_sizes.csv\n",
      "\n",
      "Headline metrics:\n",
      " - Accuracy:  0.620\n",
      " - Precision: 0.455\n",
      " - Recall:    0.180\n",
      "\n",
      "Fairness (baseline):\n",
      " - Disparate Impact (g1/g0): 1.0408850408850407\n",
      " - Equal Opportunity Diff:   0.025\n",
      "\n",
      "Fairness (mitigated demo):\n",
      " - Thresholds: group0=0.45, group1=0.45\n",
      " - Disparate Impact (g1/g0): 1.0027314408350405\n",
      " - Equal Opportunity Diff:   0.024\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "AI Ethics in Action â€” Privacy, Transparency, Accountability, Fairness (PTAF)\n",
    "\n",
    "This script is intentionally written as a \"teaching + portfolio\" artifact:\n",
    "- It is runnable end-to-end\n",
    "- It exports artifacts to ./artifacts so you can share results on GitHub\n",
    "- It includes detailed, practical comments explaining WHY each step exists\n",
    "\n",
    "What you get:\n",
    "  1) A synthetic dataset with direct identifiers + quasi-identifiers + a sensitive attribute\n",
    "  2) Privacy controls:\n",
    "       - pseudonymization (salted hashing of identifiers)\n",
    "       - generalization of quasi-identifiers (k-anonymity style check)\n",
    "       - DP-like noisy aggregates (concept demo)\n",
    "  3) Transparency artifacts:\n",
    "       - interpretable logistic regression coefficients\n",
    "       - per-decision \"reason codes\"\n",
    "       - a simple model card (documentation)\n",
    "  4) Accountability artifacts:\n",
    "       - audit log (JSONL)\n",
    "       - dataset/model fingerprints\n",
    "       - a human-in-the-loop escalation policy\n",
    "  5) Fairness evaluation and a simple mitigation demo:\n",
    "       - subgroup metrics\n",
    "       - disparate impact ratio\n",
    "       - equal opportunity difference\n",
    "       - threshold tuning (demo mitigation)\n",
    "\n",
    "Run:\n",
    "  python ethics_in_action.py\n",
    "\n",
    "Optional:\n",
    "  python ethics_in_action.py --n 6000 --seed 7 --threshold 0.55\n",
    "\n",
    "Dependencies:\n",
    "  pip install numpy pandas scikit-learn\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import hashlib\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "# ======================================================================================\n",
    "# Utilities\n",
    "# ======================================================================================\n",
    "\n",
    "def now_iso() -> str:\n",
    "    \"\"\"\n",
    "    Return the current timestamp in ISO 8601 format.\n",
    "\n",
    "    Why it exists:\n",
    "    - When you write artifacts (model cards, fairness reports, audit logs),\n",
    "      timestamps are essential for traceability and auditing.\n",
    "    - ISO format is consistent across systems and human-readable.\n",
    "\n",
    "    Example output:\n",
    "    - \"2026-01-03T22:15:10\"\n",
    "    \"\"\"\n",
    "    return datetime.now().isoformat(timespec=\"seconds\")\n",
    "\n",
    "\n",
    "def ensure_dir(path: Path) -> None:\n",
    "    \"\"\"\n",
    "    Create the directory if it doesn't exist.\n",
    "\n",
    "    Why it exists:\n",
    "    - The script writes many outputs (CSVs/JSONs/logs). GitHub projects typically\n",
    "      store these under a single folder like ./artifacts/.\n",
    "    - This helper makes the script safe to run even if the folder isn't present.\n",
    "    \"\"\"\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def write_json(path: Path, payload: Any) -> None:\n",
    "    \"\"\"\n",
    "    Write a Python object to disk as pretty-printed JSON.\n",
    "\n",
    "    Why it exists:\n",
    "    - JSON is easy to read, share, and diff in GitHub.\n",
    "    - 'ensure_ascii=False' preserves readable text (not escaped unicode).\n",
    "    \"\"\"\n",
    "    path.write_text(json.dumps(payload, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "def write_jsonl(path: Path, events: List[Dict[str, Any]]) -> None:\n",
    "    \"\"\"\n",
    "    Append events to a JSON Lines file (one JSON record per line).\n",
    "\n",
    "    Why JSONL (not JSON)?\n",
    "    - Audit logs are typically append-only.\n",
    "    - JSONL is ideal for log pipelines: each line is self-contained and can be streamed.\n",
    "\n",
    "    Example:\n",
    "    {\"timestamp\": \"...\", \"event_type\": \"...\", \"payload\": {...}}\n",
    "    {\"timestamp\": \"...\", \"event_type\": \"...\", \"payload\": {...}}\n",
    "    \"\"\"\n",
    "    with path.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        for e in events:\n",
    "            f.write(json.dumps(e, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "\n",
    "# ======================================================================================\n",
    "# Data Generation (Synthetic)\n",
    "# ======================================================================================\n",
    "\n",
    "def make_synthetic_hiring_data(n: int, seed: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a synthetic dataset that resembles a simple \"hiring screening\" scenario.\n",
    "\n",
    "    Purpose:\n",
    "    - We need a dataset we can publish publicly without privacy risk.\n",
    "    - We intentionally include:\n",
    "        (a) direct identifiers: name, email\n",
    "        (b) quasi-identifiers: age, zipcode, education\n",
    "        (c) a sensitive attribute: group (0/1) used ONLY for fairness evaluation\n",
    "        (d) a label: recommended_for_review (binary outcome)\n",
    "\n",
    "    Important ethics note:\n",
    "    - The label generation deliberately injects mild bias to show how fairness issues\n",
    "      can appear even when you do not use the sensitive attribute in the model features.\n",
    "\n",
    "    Columns:\n",
    "    - name/email: direct identifiers (must be removed for privacy)\n",
    "    - age/zipcode/education: quasi-identifiers (re-identification risk)\n",
    "    - experience_years/skill_score: predictive features\n",
    "    - group: sensitive attribute used only to measure fairness\n",
    "    - recommended_for_review: outcome label used for training/evaluation\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # Quasi-identifiers\n",
    "    age = rng.integers(21, 60, size=n)\n",
    "    zipcode = rng.choice([10001, 10002, 10003, 10004, 10005, 10006, 10007, 10008], size=n)\n",
    "    education = rng.choice(\n",
    "        [\"HighSchool\", \"Bachelors\", \"Masters\", \"PhD\"],\n",
    "        size=n,\n",
    "        p=[0.20, 0.45, 0.28, 0.07],\n",
    "    )\n",
    "\n",
    "    # Predictive features (constructed with some randomness)\n",
    "    experience_years = np.clip(rng.normal(loc=(age - 21) / 3, scale=2.5, size=n), 0, 30)\n",
    "    skill_score = np.clip(rng.normal(loc=65, scale=12, size=n), 0, 100)\n",
    "\n",
    "    # Sensitive attribute: think of this as a protected group indicator (demo only)\n",
    "    group = rng.binomial(1, 0.45, size=n)\n",
    "\n",
    "    # Encode education into an ordinal-ish number for label generation\n",
    "    edu_map = {\"HighSchool\": 0, \"Bachelors\": 1, \"Masters\": 2, \"PhD\": 3}\n",
    "    edu_num = np.array([edu_map[e] for e in education])\n",
    "\n",
    "    # \"Legitimate\" signal: skill, experience, education affect label probability\n",
    "    base = 0.03 * skill_score + 0.09 * experience_years + 0.25 * edu_num\n",
    "\n",
    "    # Bias injection (for fairness demonstration):\n",
    "    # We subtract a small amount for group==1, simulating historical bias.\n",
    "    biased_score = base - 0.25 * group + rng.normal(0, 0.8, size=n)\n",
    "\n",
    "    # Convert to probability via logistic function and sample binary label\n",
    "    prob = 1 / (1 + np.exp(-(biased_score - 3.4)))\n",
    "    y = rng.binomial(1, prob, size=n)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"name\": [f\"Candidate_{i}\" for i in range(n)],\n",
    "            \"email\": [f\"user{i}@example.com\" for i in range(n)],\n",
    "            \"age\": age,\n",
    "            \"zipcode\": zipcode,\n",
    "            \"education\": education,\n",
    "            \"experience_years\": np.round(experience_years, 1),\n",
    "            \"skill_score\": np.round(skill_score, 1),\n",
    "            \"group\": group,\n",
    "            \"recommended_for_review\": y,\n",
    "        }\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "# ======================================================================================\n",
    "# Privacy\n",
    "# ======================================================================================\n",
    "\n",
    "def pseudonymize(df: pd.DataFrame, salt: str, id_cols: Tuple[str, ...] = (\"name\", \"email\")) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Privacy Control #1: Pseudonymization\n",
    "\n",
    "    What it does:\n",
    "    - Creates a stable pseudo_id derived from the user's email (salted hash).\n",
    "    - Drops direct identifiers like name and email.\n",
    "\n",
    "    Why this matters:\n",
    "    - Direct identifiers (name/email) are high-risk because they uniquely identify people.\n",
    "    - Pseudonymization lowers risk while still allowing:\n",
    "        - record linking inside the system\n",
    "        - debugging/tracing without storing raw PII\n",
    "\n",
    "    Why salted hashing:\n",
    "    - Hashing without a salt is vulnerable to \"dictionary attacks\" (guess common emails).\n",
    "    - Salt ensures that even if someone knows an email, they cannot easily reproduce its hash\n",
    "      without the salt.\n",
    "\n",
    "    Output:\n",
    "    - pseudo_id (string)\n",
    "    - all non-identifier columns preserved\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "\n",
    "    def pid(email: str) -> str:\n",
    "        return hashlib.sha256((salt + str(email)).encode(\"utf-8\")).hexdigest()[:12]\n",
    "\n",
    "    out[\"pseudo_id\"] = out[\"email\"].map(pid)\n",
    "    out = out.drop(columns=list(id_cols))\n",
    "\n",
    "    # Put pseudo_id first for readability\n",
    "    cols = [\"pseudo_id\"] + [c for c in out.columns if c != \"pseudo_id\"]\n",
    "    return out[cols]\n",
    "\n",
    "\n",
    "def generalize_quasi_identifiers(\n",
    "    df: pd.DataFrame,\n",
    "    k: int = 15,\n",
    "    quasi: Tuple[str, ...] = (\"age\", \"zipcode\", \"education\"),\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Privacy Control #2: Generalization of quasi-identifiers (k-anonymity style check)\n",
    "\n",
    "    Background:\n",
    "    - Even if you remove direct identifiers, people can sometimes be re-identified\n",
    "      using combinations of quasi-identifiers like:\n",
    "        (age, zipcode, education)\n",
    "\n",
    "    What we do:\n",
    "    - age -> age_band (5-year bucket)\n",
    "    - zipcode -> zip_prefix (first 3 digits + \"**\")\n",
    "    - education stays (in real systems, it may also be generalized depending on risk)\n",
    "\n",
    "    Then we compute:\n",
    "    - group sizes for each (age_band, zip_prefix, education) combination\n",
    "\n",
    "    How to interpret k:\n",
    "    - k=15 means \"each unique quasi-identifier combination should have at least 15 records\"\n",
    "      to reduce re-identification risk.\n",
    "    - If many groups have counts below k, your data is not safe to release at that granularity.\n",
    "\n",
    "    Returns:\n",
    "    - generalized dataframe (safer for sharing)\n",
    "    - group_sizes table (so you can see how many groups fall below k)\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "\n",
    "    # Generalize age: 23 -> \"20-24\", 38 -> \"35-39\", etc.\n",
    "    out[\"age_band_start\"] = (out[\"age\"] // 5) * 5\n",
    "    out[\"age_band\"] = out[\"age_band_start\"].astype(str) + \"-\" + (out[\"age_band_start\"] + 4).astype(str)\n",
    "\n",
    "    # Generalize zipcode: 10006 -> \"100**\"\n",
    "    out[\"zip_prefix\"] = out[\"zipcode\"].astype(str).str[:3] + \"**\"\n",
    "\n",
    "    # Drop precise values that increase re-identification risk\n",
    "    out = out.drop(columns=[\"age\", \"zipcode\", \"age_band_start\"])\n",
    "\n",
    "    # Count group sizes across quasi identifier combinations\n",
    "    group_sizes = (\n",
    "        out.groupby([\"age_band\", \"zip_prefix\", \"education\"])\n",
    "        .size()\n",
    "        .reset_index(name=\"count\")\n",
    "        .sort_values(\"count\")\n",
    "    )\n",
    "    return out, group_sizes\n",
    "\n",
    "\n",
    "def dp_like_mean(values: np.ndarray, epsilon: float, value_range: Tuple[float, float], seed: int) -> float:\n",
    "    \"\"\"\n",
    "    Privacy Control #3: DP-like noisy aggregate release (educational demo)\n",
    "\n",
    "    What problem this addresses:\n",
    "    - Sharing aggregates like \"average skill score by group\" can leak information,\n",
    "      especially for small groups.\n",
    "    - Differential Privacy (DP) is a formal approach where you add noise calibrated\n",
    "      to how much a single individual's data can change the output.\n",
    "\n",
    "    What this function implements:\n",
    "    - A teaching approximation for the DP mean using Laplace noise.\n",
    "    - Sensitivity of mean ~ (max-min)/n\n",
    "      Explanation:\n",
    "        If one person's value changes from min to max, the mean can shift by (max-min)/n\n",
    "\n",
    "    Noise:\n",
    "    - Laplace noise scale = sensitivity / epsilon\n",
    "    - Larger epsilon -> less noise (less privacy, more accuracy)\n",
    "    - Smaller epsilon -> more noise (more privacy, less accuracy)\n",
    "\n",
    "    Important disclaimer:\n",
    "    - This is NOT a production DP system.\n",
    "    - Production DP requires careful accounting, composition rules, and privacy budgets.\n",
    "\n",
    "    Returns:\n",
    "    - noisy mean that demonstrates the concept of privacy-preserving aggregate sharing\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    values = np.asarray(values, dtype=float)\n",
    "    n = max(len(values), 1)\n",
    "\n",
    "    sensitivity = (value_range[1] - value_range[0]) / n\n",
    "    noise = rng.laplace(loc=0.0, scale=sensitivity / max(epsilon, 1e-9))\n",
    "    return float(values.mean() + noise)\n",
    "\n",
    "\n",
    "# ======================================================================================\n",
    "# Transparency\n",
    "# ======================================================================================\n",
    "\n",
    "def build_pipeline(cat_cols: List[str], num_cols: List[str]) -> Pipeline:\n",
    "    \"\"\"\n",
    "    Build a simple, interpretable ML pipeline.\n",
    "\n",
    "    Why a pipeline:\n",
    "    - Ensures preprocessing and modeling are bundled consistently.\n",
    "    - Prevents \"training vs inference mismatch\" (a real deployment issue).\n",
    "\n",
    "    Why logistic regression:\n",
    "    - Interpretable: coefficients tell direction + strength of feature influence.\n",
    "    - Fast and stable: good for demos and governance-driven systems.\n",
    "    - Easy to explain to non-technical stakeholders.\n",
    "\n",
    "    Preprocessing:\n",
    "    - Categorical features -> one-hot encoding\n",
    "    - Numeric features -> passed through unchanged\n",
    "    \"\"\"\n",
    "    pre = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "            (\"num\", \"passthrough\", num_cols),\n",
    "        ]\n",
    "    )\n",
    "    clf = LogisticRegression(max_iter=2000)\n",
    "    return Pipeline([(\"prep\", pre), (\"clf\", clf)])\n",
    "\n",
    "\n",
    "def feature_names_from_pipeline(pipe: Pipeline, cat_cols: List[str], num_cols: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Recover the expanded feature names after preprocessing.\n",
    "\n",
    "    Why this exists:\n",
    "    - One-hot encoding expands a single categorical column into multiple features.\n",
    "      Example: education -> education_Bachelors, education_Masters, etc.\n",
    "    - To interpret model coefficients, we must know which expanded features exist.\n",
    "    \"\"\"\n",
    "    pre: ColumnTransformer = pipe.named_steps[\"prep\"]\n",
    "    ohe: OneHotEncoder = pre.named_transformers_[\"cat\"]\n",
    "    cat_names = list(ohe.get_feature_names_out(cat_cols))\n",
    "    return cat_names + num_cols\n",
    "\n",
    "\n",
    "def coefficient_table(pipe: Pipeline, cat_cols: List[str], num_cols: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a ranked table of model coefficients.\n",
    "\n",
    "    What coefficients mean in logistic regression:\n",
    "    - Positive coefficient -> increases probability of label=1\n",
    "    - Negative coefficient -> decreases probability of label=1\n",
    "    - Magnitude (absolute value) -> strength of influence\n",
    "\n",
    "    Why it supports transparency:\n",
    "    - Lets reviewers see what features drive decisions.\n",
    "    - Helps identify suspicious proxies (e.g., ZIP code proxies for sensitive traits).\n",
    "\n",
    "    Output:\n",
    "    - feature: transformed feature name\n",
    "    - coef: signed coefficient\n",
    "    - abs_coef: magnitude used for ranking\n",
    "    \"\"\"\n",
    "    names = feature_names_from_pipeline(pipe, cat_cols, num_cols)\n",
    "    coefs = pipe.named_steps[\"clf\"].coef_[0]\n",
    "    tbl = pd.DataFrame({\"feature\": names, \"coef\": coefs, \"abs_coef\": np.abs(coefs)})\n",
    "    return tbl.sort_values(\"abs_coef\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def reason_codes(pipe: Pipeline, row_df: pd.DataFrame, cat_cols: List[str], num_cols: List[str], top_k: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate simple \"reason codes\" for a single prediction.\n",
    "\n",
    "    Goal:\n",
    "    - Provide an explanation per decision: \"Which factors mattered most?\"\n",
    "\n",
    "    How it works (approximation):\n",
    "    - Logistic regression is linear in feature space.\n",
    "    - After preprocessing, each feature has a value x_i and coefficient w_i.\n",
    "    - Contribution ~ x_i * w_i\n",
    "    - We rank contributions by absolute magnitude.\n",
    "\n",
    "    Why this is useful:\n",
    "    - Supports transparency at the individual decision level.\n",
    "    - Helps debugging and stakeholder explanations.\n",
    "    - Helps identify unfair feature effects in practice.\n",
    "\n",
    "    Output:\n",
    "    - top_k transformed features with highest influence for this specific row\n",
    "    \"\"\"\n",
    "    pre: ColumnTransformer = pipe.named_steps[\"prep\"]\n",
    "    clf: LogisticRegression = pipe.named_steps[\"clf\"]\n",
    "\n",
    "    x_vec = pre.transform(row_df)\n",
    "    x_dense = x_vec.toarray() if hasattr(x_vec, \"toarray\") else np.asarray(x_vec)\n",
    "\n",
    "    contrib = x_dense[0] * clf.coef_[0]\n",
    "    names = feature_names_from_pipeline(pipe, cat_cols, num_cols)\n",
    "\n",
    "    out = pd.DataFrame({\"feature\": names, \"contribution\": contrib, \"abs\": np.abs(contrib)})\n",
    "    return out.sort_values(\"abs\", ascending=False).head(top_k)[[\"feature\", \"contribution\"]].reset_index(drop=True)\n",
    "\n",
    "\n",
    "def make_model_card(\n",
    "    model_name: str,\n",
    "    intended_use: str,\n",
    "    limitations: List[str],\n",
    "    data_notes: str,\n",
    "    metrics: Dict[str, float],\n",
    "    ethical_notes: Dict[str, str],\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Create a lightweight model card (documentation artifact).\n",
    "\n",
    "    Why model cards matter (Transparency + Accountability):\n",
    "    - They document:\n",
    "        - what the model is for (and not for)\n",
    "        - what data it was trained on\n",
    "        - performance metrics\n",
    "        - known limitations and risks\n",
    "    - In real systems, this is essential for governance and responsible deployment.\n",
    "\n",
    "    Output:\n",
    "    - A JSON-serializable dictionary stored in artifacts/model_card.json\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"model_name\": model_name,\n",
    "        \"created_at\": now_iso(),\n",
    "        \"intended_use\": intended_use,\n",
    "        \"data_notes\": data_notes,\n",
    "        \"limitations\": limitations,\n",
    "        \"evaluation_metrics\": metrics,\n",
    "        \"ethical_notes\": ethical_notes,\n",
    "    }\n",
    "\n",
    "\n",
    "# ======================================================================================\n",
    "# Accountability\n",
    "# ======================================================================================\n",
    "\n",
    "def df_fingerprint(df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Create a fingerprint (hash) of a dataframe's content.\n",
    "\n",
    "    Why it matters:\n",
    "    - Accountability requires traceability: \"Which dataset version trained this model?\"\n",
    "    - A fingerprint lets you later prove:\n",
    "        - whether the training data changed\n",
    "        - whether a model was trained on the expected data\n",
    "\n",
    "    How it's done:\n",
    "    - hash_pandas_object creates row-wise hashes\n",
    "    - we hash the bytes using SHA-256 to get a stable signature\n",
    "    \"\"\"\n",
    "    h = pd.util.hash_pandas_object(df, index=True).values.tobytes()\n",
    "    return hashlib.sha256(h).hexdigest()\n",
    "\n",
    "\n",
    "def pipeline_fingerprint(pipe: Pipeline) -> str:\n",
    "    \"\"\"\n",
    "    Create a fingerprint of the model configuration.\n",
    "\n",
    "    Why it matters:\n",
    "    - You want to know if your model changed because:\n",
    "        - hyperparameters changed\n",
    "        - preprocessing steps changed\n",
    "        - encoders changed\n",
    "    - This helps with reproducibility and auditing.\n",
    "\n",
    "    What it captures:\n",
    "    - Pipeline parameters (deep=True), serialized deterministically.\n",
    "    \"\"\"\n",
    "    params = pipe.get_params(deep=True)\n",
    "    blob = json.dumps(params, sort_keys=True, default=str).encode(\"utf-8\")\n",
    "    return hashlib.sha256(blob).hexdigest()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AuditEvent:\n",
    "    \"\"\"\n",
    "    A structured audit event.\n",
    "\n",
    "    Why use structured events:\n",
    "    - Easier to parse and analyze than unstructured text logs.\n",
    "    - Consistent format makes it easier to build dashboards or compliance reports later.\n",
    "\n",
    "    Fields:\n",
    "    - timestamp: when the event occurred\n",
    "    - event_type: category (prediction, registry, hitl_policy, etc.)\n",
    "    - payload: structured details\n",
    "    \"\"\"\n",
    "    timestamp: str\n",
    "    event_type: str\n",
    "    payload: Dict[str, Any]\n",
    "\n",
    "\n",
    "class Auditor:\n",
    "    \"\"\"\n",
    "    An append-only auditor.\n",
    "\n",
    "    How it works:\n",
    "    - Buffer events in memory (self.buffer)\n",
    "    - Flush them to JSONL (append-only) for traceability\n",
    "\n",
    "    Why append-only:\n",
    "    - Real audit logs should be difficult to tamper with.\n",
    "    - JSONL enables streaming and event-based tooling.\n",
    "    \"\"\"\n",
    "    def __init__(self, audit_path: Path):\n",
    "        self.audit_path = audit_path\n",
    "        self.buffer: List[Dict[str, Any]] = []\n",
    "\n",
    "    def log(self, event_type: str, payload: Dict[str, Any]) -> None:\n",
    "        \"\"\"Add an event to the buffer.\"\"\"\n",
    "        self.buffer.append({\"timestamp\": now_iso(), \"event_type\": event_type, \"payload\": payload})\n",
    "\n",
    "    def flush(self) -> None:\n",
    "        \"\"\"Write buffered events to disk and clear the buffer.\"\"\"\n",
    "        if self.buffer:\n",
    "            write_jsonl(self.audit_path, self.buffer)\n",
    "            self.buffer = []\n",
    "\n",
    "\n",
    "def predict_with_audit(\n",
    "    auditor: Auditor,\n",
    "    pipe: Pipeline,\n",
    "    row_df: pd.DataFrame,\n",
    "    cat_cols: List[str],\n",
    "    num_cols: List[str],\n",
    "    reviewer: str,\n",
    "    threshold: float,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Make a prediction and produce an audit record.\n",
    "\n",
    "    What it logs:\n",
    "    - who requested the prediction (reviewer)\n",
    "    - which threshold was used (policy parameter)\n",
    "    - probability and resulting decision\n",
    "    - input features (what was evaluated)\n",
    "    - reason codes (why the model leaned that way)\n",
    "\n",
    "    Why it matters:\n",
    "    - In accountable AI systems, you need to reconstruct:\n",
    "        - what was predicted\n",
    "        - why it was predicted\n",
    "        - under which policy rules\n",
    "\n",
    "    Return:\n",
    "    - dictionary containing probability, decision, reason codes (useful for UI/logging)\n",
    "    \"\"\"\n",
    "    p = float(pipe.predict_proba(row_df)[0, 1])\n",
    "    decision = \"REVIEW\" if p >= threshold else \"NO_REVIEW\"\n",
    "    reasons = reason_codes(pipe, row_df, cat_cols, num_cols, top_k=4).to_dict(orient=\"records\")\n",
    "\n",
    "    auditor.log(\n",
    "        \"prediction\",\n",
    "        {\n",
    "            \"reviewer\": reviewer,\n",
    "            \"threshold\": threshold,\n",
    "            \"probability\": p,\n",
    "            \"decision\": decision,\n",
    "            \"input_features\": row_df.to_dict(orient=\"records\")[0],\n",
    "            \"reason_codes\": reasons,\n",
    "        },\n",
    "    )\n",
    "    return {\"probability\": p, \"decision\": decision, \"reason_codes\": reasons}\n",
    "\n",
    "\n",
    "def hitl_policy(probability: float, low: float = 0.45, high: float = 0.65) -> str:\n",
    "    \"\"\"\n",
    "    Human-in-the-loop (HITL) policy.\n",
    "\n",
    "    Why HITL is important:\n",
    "    - Many decisions should NOT be fully automated (especially high-impact decisions).\n",
    "    - A common pattern is:\n",
    "        - Auto-decide when confidence is high\n",
    "        - Escalate ambiguous cases to humans\n",
    "\n",
    "    Policy:\n",
    "    - probability < low  -> AUTO_NO_REVIEW\n",
    "    - probability > high -> AUTO_REVIEW\n",
    "    - otherwise          -> ESCALATE_TO_HUMAN\n",
    "\n",
    "    This demonstrates accountability because:\n",
    "    - it shows explicit decision governance instead of \"model decides everything.\"\n",
    "    \"\"\"\n",
    "    if probability < low:\n",
    "        return \"AUTO_NO_REVIEW\"\n",
    "    if probability > high:\n",
    "        return \"AUTO_REVIEW\"\n",
    "    return \"ESCALATE_TO_HUMAN\"\n",
    "\n",
    "\n",
    "# ======================================================================================\n",
    "# Fairness\n",
    "# ======================================================================================\n",
    "\n",
    "def subgroup_report(y_true: np.ndarray, y_pred: np.ndarray, sensitive: np.ndarray) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute performance metrics separately for each sensitive group.\n",
    "\n",
    "    Why subgroup metrics matter:\n",
    "    - Overall accuracy can hide unequal performance.\n",
    "    - A model can appear \"good\" on average while harming a subgroup.\n",
    "\n",
    "    Metrics produced:\n",
    "    - accuracy: correct predictions / total\n",
    "    - precision: among predicted positives, how many were actually positive?\n",
    "    - recall (TPR): among actual positives, how many did we catch?\n",
    "    - selection_rate: fraction predicted positive (important for impact)\n",
    "\n",
    "    selection_rate is critical for fairness:\n",
    "    - If one group receives far fewer positive outcomes, it may indicate disparate impact.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for g in sorted(np.unique(sensitive)):\n",
    "        idx = sensitive == g\n",
    "        rows.append(\n",
    "            {\n",
    "                \"group\": int(g),\n",
    "                \"n\": int(idx.sum()),\n",
    "                \"accuracy\": float(accuracy_score(y_true[idx], y_pred[idx])),\n",
    "                \"precision\": float(precision_score(y_true[idx], y_pred[idx], zero_division=0)),\n",
    "                \"recall\": float(recall_score(y_true[idx], y_pred[idx], zero_division=0)),\n",
    "                \"selection_rate\": float(y_pred[idx].mean()),\n",
    "            }\n",
    "        )\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def disparate_impact(y_pred: np.ndarray, sensitive: np.ndarray) -> Tuple[Optional[float], Dict[int, float]]:\n",
    "    \"\"\"\n",
    "    Compute the Disparate Impact (DI) ratio.\n",
    "\n",
    "    What DI measures:\n",
    "    - DI compares selection rates between groups.\n",
    "    - selection_rate(group) = P(predicted_positive | group)\n",
    "\n",
    "    DI ratio commonly reported as:\n",
    "    - DI = selection_rate(group_1) / selection_rate(group_0)\n",
    "\n",
    "    Interpretation:\n",
    "    - DI ~= 1.0  -> both groups selected at similar rates\n",
    "    - DI < 1.0   -> group_1 is selected less often than group_0\n",
    "    - DI > 1.0   -> group_1 is selected more often than group_0\n",
    "\n",
    "    Why it matters:\n",
    "    - DI is a widely used \"impact\" fairness metric (it does NOT prove discrimination,\n",
    "      but it is a strong signal to investigate).\n",
    "    - In some compliance contexts, values far from 1.0 raise flags.\n",
    "\n",
    "    Return:\n",
    "    - (di_ratio, selection_rates)\n",
    "      - di_ratio is None if division by zero occurs\n",
    "      - selection_rates is a dict like {0: 0.42, 1: 0.31}\n",
    "    \"\"\"\n",
    "    rates: Dict[int, float] = {}\n",
    "    for g in sorted(np.unique(sensitive)):\n",
    "        idx = sensitive == g\n",
    "        rates[int(g)] = float(y_pred[idx].mean())\n",
    "\n",
    "    # Protect against division by zero (if group0 selection rate is 0)\n",
    "    if 0 in rates and rates[0] > 0 and 1 in rates:\n",
    "        return float(rates[1] / rates[0]), rates\n",
    "    return None, rates\n",
    "\n",
    "\n",
    "def equal_opportunity_diff(y_true: np.ndarray, y_pred: np.ndarray, sensitive: np.ndarray) -> Tuple[float, Dict[int, float]]:\n",
    "    \"\"\"\n",
    "    Compute Equal Opportunity Difference (EOD).\n",
    "\n",
    "    What EOD measures:\n",
    "    - Equal opportunity focuses on true positive rate (TPR / recall).\n",
    "    - TPR(group) = P(predicted_positive | actually_positive, group)\n",
    "\n",
    "    EOD:\n",
    "    - EOD = TPR(group_1) - TPR(group_0)\n",
    "\n",
    "    Interpretation:\n",
    "    - EOD ~= 0.0 -> both groups get positives recognized at similar rates\n",
    "    - Positive EOD -> group_1 has higher TPR than group_0\n",
    "    - Negative EOD -> group_1 has lower TPR (more missed positives)\n",
    "\n",
    "    Why it matters:\n",
    "    - DI focuses on outcome rates (impact).\n",
    "    - Equal opportunity focuses on errors among the truly eligible/positive cases.\n",
    "    - In many high-stakes settings, error parity is as important as outcome parity.\n",
    "\n",
    "    Return:\n",
    "    - (difference, tpr_by_group)\n",
    "    \"\"\"\n",
    "    tpr: Dict[int, float] = {}\n",
    "    for g in sorted(np.unique(sensitive)):\n",
    "        idx = sensitive == g\n",
    "        tpr[int(g)] = float(recall_score(y_true[idx], y_pred[idx], zero_division=0))\n",
    "    return float(tpr.get(1, 0.0) - tpr.get(0, 0.0)), tpr\n",
    "\n",
    "\n",
    "def apply_group_thresholds(proba: np.ndarray, sensitive: np.ndarray, t0: float, t1: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Apply different decision thresholds per group.\n",
    "\n",
    "    Why this exists:\n",
    "    - It's a simple demonstration of a fairness mitigation approach:\n",
    "        \"change the decision threshold to reduce disparity.\"\n",
    "\n",
    "    Important warning:\n",
    "    - Using group-specific thresholds is a sensitive governance decision.\n",
    "    - In real systems, this must be reviewed legally and ethically because:\n",
    "        - you are explicitly using group membership during decision-making\n",
    "\n",
    "    How it works:\n",
    "    - For group==0, positive if probability >= t0\n",
    "    - For group==1, positive if probability >= t1\n",
    "\n",
    "    Returns:\n",
    "    - binary predictions (0/1)\n",
    "    \"\"\"\n",
    "    out = np.zeros_like(proba, dtype=int)\n",
    "    out[sensitive == 0] = (proba[sensitive == 0] >= t0).astype(int)\n",
    "    out[sensitive == 1] = (proba[sensitive == 1] >= t1).astype(int)\n",
    "    return out\n",
    "\n",
    "\n",
    "def search_thresholds(\n",
    "    proba: np.ndarray,\n",
    "    y_true: np.ndarray,\n",
    "    sensitive: np.ndarray,\n",
    "    grid: np.ndarray,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Search for thresholds (t0, t1) that reduce DI disparity while keeping accuracy reasonable.\n",
    "\n",
    "    Why grid search:\n",
    "    - We want an understandable mitigation method.\n",
    "    - You can inspect and explain the search space easily.\n",
    "\n",
    "    Objective used (demo):\n",
    "    - score = -abs(DI - 1.0) + 0.15 * accuracy\n",
    "      Explanation:\n",
    "        - push DI toward 1.0 (reduce selection disparity)\n",
    "        - still reward decent accuracy so we don't \"fix fairness\" by making the model useless\n",
    "\n",
    "    Output:\n",
    "    - dict with best thresholds and achieved metrics:\n",
    "        {\"t0\": ..., \"t1\": ..., \"di\": ..., \"acc\": ..., \"score\": ...}\n",
    "    \"\"\"\n",
    "    best: Optional[Dict[str, float]] = None\n",
    "    for t0 in grid:\n",
    "        for t1 in grid:\n",
    "            pred = apply_group_thresholds(proba, sensitive, float(t0), float(t1))\n",
    "            di, _ = disparate_impact(pred, sensitive)\n",
    "            if di is None:\n",
    "                continue\n",
    "\n",
    "            acc = float(accuracy_score(y_true, pred))\n",
    "\n",
    "            # Demo objective: prioritize DI closeness to 1.0, lightly preserve accuracy\n",
    "            score = -abs(di - 1.0) + 0.15 * acc\n",
    "\n",
    "            cand = {\"t0\": float(t0), \"t1\": float(t1), \"di\": float(di), \"acc\": acc, \"score\": float(score)}\n",
    "            if best is None or cand[\"score\"] > best[\"score\"]:\n",
    "                best = cand\n",
    "\n",
    "    return best or {\"t0\": 0.5, \"t1\": 0.5, \"di\": float(\"nan\"), \"acc\": 0.0, \"score\": float(\"-inf\")}\n",
    "\n",
    "\n",
    "# ======================================================================================\n",
    "# Main workflow\n",
    "# ======================================================================================\n",
    "\n",
    "def main() -> None:\n",
    "    \"\"\"\n",
    "    Orchestrates the end-to-end PTAF workflow.\n",
    "\n",
    "    Major steps:\n",
    "    1) Generate synthetic data\n",
    "    2) Apply privacy protections and export privacy artifacts\n",
    "    3) Train an interpretable model and export transparency artifacts\n",
    "    4) Log accountability artifacts (audit log + fingerprints + HITL policy)\n",
    "    5) Evaluate fairness and export baseline + mitigated reports\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"AI Ethics in Action (PTAF) demo.\")\n",
    "    parser.add_argument(\"--n\", type=int, default=4000, help=\"Number of rows to generate.\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=42, help=\"Random seed.\")\n",
    "    parser.add_argument(\"--threshold\", type=float, default=0.50, help=\"Decision threshold for REVIEW.\")\n",
    "    parser.add_argument(\"--epsilon\", type=float, default=0.8, help=\"DP-like epsilon for aggregate demos.\")\n",
    "    parser.add_argument(\"--k\", type=int, default=15, help=\"k for k-anonymity style generalization check.\")\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    artifacts = Path(\"artifacts\")\n",
    "    ensure_dir(artifacts)\n",
    "\n",
    "    # -------------------------\n",
    "    # 1) Data creation\n",
    "    # -------------------------\n",
    "    df_raw = make_synthetic_hiring_data(n=args.n, seed=args.seed)\n",
    "    df_raw.to_csv(artifacts / \"raw_data.csv\", index=False)\n",
    "\n",
    "    # -------------------------\n",
    "    # 2) Privacy protections\n",
    "    # -------------------------\n",
    "    salt = f\"PTAF_DEMO_SALT_{args.seed}\"\n",
    "    df_priv = pseudonymize(df_raw, salt=salt)\n",
    "    df_priv.to_csv(artifacts / \"pseudonymized_data.csv\", index=False)\n",
    "\n",
    "    df_gen, group_sizes = generalize_quasi_identifiers(df_priv, k=args.k)\n",
    "    df_gen.to_csv(artifacts / \"generalized_data.csv\", index=False)\n",
    "    group_sizes.to_csv(artifacts / \"k_anonymity_group_sizes.csv\", index=False)\n",
    "\n",
    "    # DP-like noisy aggregates (demo): mean skill by group\n",
    "    dp_report = []\n",
    "    for g in [0, 1]:\n",
    "        mask = df_priv[\"group\"].values == g\n",
    "        real_mean = float(df_raw.loc[mask, \"skill_score\"].mean())\n",
    "        dp_mean = dp_like_mean(\n",
    "            values=df_raw.loc[mask, \"skill_score\"].values,\n",
    "            epsilon=args.epsilon,\n",
    "            value_range=(0.0, 100.0),\n",
    "            seed=args.seed + 100 + g,\n",
    "        )\n",
    "        dp_report.append({\"group\": g, \"real_mean_skill\": real_mean, \"dp_like_mean_skill\": dp_mean})\n",
    "\n",
    "    write_json(\n",
    "        artifacts / \"dp_like_aggregates.json\",\n",
    "        {\"created_at\": now_iso(), \"epsilon\": args.epsilon, \"items\": dp_report},\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # 3) Train interpretable model\n",
    "    # -------------------------\n",
    "    # Target label\n",
    "    y = df_priv[\"recommended_for_review\"].values.astype(int)\n",
    "\n",
    "    # Sensitive attribute used ONLY for fairness evaluation\n",
    "    sensitive = df_priv[\"group\"].values.astype(int)\n",
    "\n",
    "    # Feature set excludes:\n",
    "    # - pseudo_id: identifier-like (not predictive, and increases linkage risk)\n",
    "    # - group: sensitive attribute (excluded to show fairness issues can still appear via proxies)\n",
    "    X = df_priv.drop(columns=[\"recommended_for_review\", \"pseudo_id\", \"group\"])\n",
    "\n",
    "    cat_cols = [\"education\"]\n",
    "    num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "\n",
    "    X_train, X_test, y_train, y_test, s_train, s_test = train_test_split(\n",
    "        X, y, sensitive, test_size=0.25, random_state=args.seed, stratify=y\n",
    "    )\n",
    "\n",
    "    pipe = build_pipeline(cat_cols, num_cols)\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    proba = pipe.predict_proba(X_test)[:, 1]\n",
    "    pred = (proba >= args.threshold).astype(int)\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": float(accuracy_score(y_test, pred)),\n",
    "        \"precision\": float(precision_score(y_test, pred)),\n",
    "        \"recall\": float(recall_score(y_test, pred)),\n",
    "        \"threshold\": float(args.threshold),\n",
    "    }\n",
    "    write_json(artifacts / \"metrics.json\", {\"created_at\": now_iso(), \"metrics\": metrics})\n",
    "\n",
    "    # Transparency artifacts\n",
    "    coef_tbl = coefficient_table(pipe, cat_cols, num_cols)\n",
    "    coef_tbl.to_csv(artifacts / \"coefficients.csv\", index=False)\n",
    "\n",
    "    explanations = []\n",
    "    sample_indices = np.linspace(0, len(X_test) - 1, num=min(12, len(X_test)), dtype=int)\n",
    "    for idx in sample_indices:\n",
    "        row = X_test.iloc[[idx]]\n",
    "        p = float(pipe.predict_proba(row)[0, 1])\n",
    "        reasons_tbl = reason_codes(pipe, row, cat_cols, num_cols, top_k=5)\n",
    "        explanations.append(\n",
    "            {\n",
    "                \"row_index\": int(idx),\n",
    "                \"probability\": p,\n",
    "                \"decision\": \"REVIEW\" if p >= args.threshold else \"NO_REVIEW\",\n",
    "                \"reason_codes\": reasons_tbl.to_dict(orient=\"records\"),\n",
    "                \"input\": row.to_dict(orient=\"records\")[0],\n",
    "            }\n",
    "        )\n",
    "    write_json(artifacts / \"sample_explanations.json\", {\"created_at\": now_iso(), \"items\": explanations})\n",
    "\n",
    "    model_card = make_model_card(\n",
    "        model_name=\"HiringScreen-PTAF-v1 (LogisticRegression)\",\n",
    "        intended_use=\"Assist with screening by recommending candidates for human review (not an automated hiring decision).\",\n",
    "        limitations=[\n",
    "            \"Synthetic dataset used for demonstration; not representative of real labor markets.\",\n",
    "            \"Model behavior depends on feature definitions; requires domain review before any real use.\",\n",
    "            \"Interpretability is limited to linear contributions; does not capture complex interactions.\",\n",
    "        ],\n",
    "        data_notes=\"Trained on education, experience_years, skill_score; direct identifiers removed; sensitive attribute used for evaluation only.\",\n",
    "        metrics=metrics,\n",
    "        ethical_notes={\n",
    "            \"privacy\": \"Direct identifiers removed; pseudonymous IDs used; quasi-identifiers generalized for sharing.\",\n",
    "            \"transparency\": \"Coefficients and per-decision reason codes exported; model card included.\",\n",
    "            \"accountability\": \"Audit logs + dataset/model fingerprints; HITL escalation policy demonstrated.\",\n",
    "            \"fairness\": \"Subgroup metrics, disparate impact, and equal opportunity reported; threshold mitigation demonstrated.\",\n",
    "        },\n",
    "    )\n",
    "    write_json(artifacts / \"model_card.json\", model_card)\n",
    "\n",
    "    # -------------------------\n",
    "    # 4) Accountability artifacts\n",
    "    # -------------------------\n",
    "    auditor = Auditor(audit_path=artifacts / \"audit_log.jsonl\")\n",
    "\n",
    "    train_fprint = df_fingerprint(\n",
    "        pd.concat([X_train.reset_index(drop=True), pd.Series(y_train, name=\"label\")], axis=1)\n",
    "    )\n",
    "    model_fprint = pipeline_fingerprint(pipe)\n",
    "\n",
    "    auditor.log(\n",
    "        \"model_registry\",\n",
    "        {\n",
    "            \"model_name\": model_card[\"model_name\"],\n",
    "            \"dataset_fingerprint\": train_fprint,\n",
    "            \"model_fingerprint\": model_fprint,\n",
    "            \"metrics\": metrics,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Log a few audited predictions + HITL decisions\n",
    "    for idx in sample_indices[:6]:\n",
    "        row = X_test.iloc[[idx]]\n",
    "        out = predict_with_audit(\n",
    "            auditor=auditor,\n",
    "            pipe=pipe,\n",
    "            row_df=row,\n",
    "            cat_cols=cat_cols,\n",
    "            num_cols=num_cols,\n",
    "            reviewer=\"demo_user\",\n",
    "            threshold=args.threshold,\n",
    "        )\n",
    "        auditor.log(\n",
    "            \"hitl_policy\",\n",
    "            {\n",
    "                \"row_index\": int(idx),\n",
    "                \"probability\": out[\"probability\"],\n",
    "                \"action\": hitl_policy(out[\"probability\"]),\n",
    "            },\n",
    "        )\n",
    "\n",
    "    auditor.flush()\n",
    "\n",
    "    # -------------------------\n",
    "    # 5) Fairness evaluation + mitigation demo\n",
    "    # -------------------------\n",
    "    base_subgroups = subgroup_report(y_test, pred, s_test)\n",
    "    base_subgroups.to_csv(artifacts / \"fairness_subgroup_metrics_baseline.csv\", index=False)\n",
    "\n",
    "    di, sel_rates = disparate_impact(pred, s_test)\n",
    "    eod, tpr = equal_opportunity_diff(y_test, pred, s_test)\n",
    "\n",
    "    fairness_summary = {\n",
    "        \"created_at\": now_iso(),\n",
    "        \"baseline\": {\n",
    "            \"threshold\": float(args.threshold),\n",
    "            \"selection_rates\": sel_rates,\n",
    "            \"disparate_impact_ratio_g1_over_g0\": di,\n",
    "            \"tpr_by_group\": tpr,\n",
    "            \"equal_opportunity_diff_g1_minus_g0\": eod,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Mitigation: group thresholds (demo). In real systems this needs governance review.\n",
    "    grid = np.linspace(0.35, 0.65, 13)\n",
    "    best = search_thresholds(proba, y_test, s_test, grid=grid)\n",
    "\n",
    "    pred_m = apply_group_thresholds(proba, s_test, best[\"t0\"], best[\"t1\"])\n",
    "    mitigated_subgroups = subgroup_report(y_test, pred_m, s_test)\n",
    "    mitigated_subgroups.to_csv(artifacts / \"fairness_subgroup_metrics_mitigated.csv\", index=False)\n",
    "\n",
    "    di2, sel_rates2 = disparate_impact(pred_m, s_test)\n",
    "    eod2, tpr2 = equal_opportunity_diff(y_test, pred_m, s_test)\n",
    "\n",
    "    fairness_summary[\"mitigated\"] = {\n",
    "        \"thresholds\": {\"group0\": best[\"t0\"], \"group1\": best[\"t1\"]},\n",
    "        \"accuracy\": float(accuracy_score(y_test, pred_m)),\n",
    "        \"selection_rates\": sel_rates2,\n",
    "        \"disparate_impact_ratio_g1_over_g0\": di2,\n",
    "        \"tpr_by_group\": tpr2,\n",
    "        \"equal_opportunity_diff_g1_minus_g0\": eod2,\n",
    "        \"search_objective\": {\"score\": best[\"score\"], \"baseline_acc\": metrics[\"accuracy\"]},\n",
    "        \"note\": \"Mitigation shown for learning; real deployment requires legal/ethical review and stakeholder approval.\",\n",
    "    }\n",
    "    write_json(artifacts / \"fairness_summary.json\", fairness_summary)\n",
    "\n",
    "    # -------------------------\n",
    "    # Console summary\n",
    "    # -------------------------\n",
    "    print(\"\\n=== PTAF Demo Complete ===\")\n",
    "    print(f\"Artifacts written to: {artifacts.resolve()}\")\n",
    "    print(\"\\nKey files:\")\n",
    "    for name in [\n",
    "        \"model_card.json\",\n",
    "        \"coefficients.csv\",\n",
    "        \"sample_explanations.json\",\n",
    "        \"audit_log.jsonl\",\n",
    "        \"fairness_summary.json\",\n",
    "        \"fairness_subgroup_metrics_baseline.csv\",\n",
    "        \"fairness_subgroup_metrics_mitigated.csv\",\n",
    "        \"dp_like_aggregates.json\",\n",
    "        \"k_anonymity_group_sizes.csv\",\n",
    "    ]:\n",
    "        p = artifacts / name\n",
    "        if p.exists():\n",
    "            print(f\" - {name}\")\n",
    "\n",
    "    print(\"\\nHeadline metrics:\")\n",
    "    print(f\" - Accuracy:  {metrics['accuracy']:.3f}\")\n",
    "    print(f\" - Precision: {metrics['precision']:.3f}\")\n",
    "    print(f\" - Recall:    {metrics['recall']:.3f}\")\n",
    "    print(\"\\nFairness (baseline):\")\n",
    "    print(f\" - Disparate Impact (g1/g0): {di if di is not None else 'n/a'}\")\n",
    "    print(f\" - Equal Opportunity Diff:   {eod:.3f}\")\n",
    "    print(\"\\nFairness (mitigated demo):\")\n",
    "    print(f\" - Thresholds: group0={best['t0']:.2f}, group1={best['t1']:.2f}\")\n",
    "    print(f\" - Disparate Impact (g1/g0): {di2 if di2 is not None else 'n/a'}\")\n",
    "    print(f\" - Equal Opportunity Diff:   {eod2:.3f}\")\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb1e238-e52e-4866-b4a1-3e6062ac67ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
